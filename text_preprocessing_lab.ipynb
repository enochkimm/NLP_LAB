{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Lab: Hotel Reviews\n",
    "\n",
    "## Overview\n",
    "\n",
    "You've recently joined TextInsight, a customer feedback analysis company that helps businesses understand customer sentiments across various platforms. As a junior data scientist, your first project involves preprocessing a dataset of hotel reviews that will be used to build a sentiment analysis model.\n",
    "\n",
    "The hotel chain wants to identify common issues mentioned in negative reviews and highlight positive experiences that could be promoted in their marketing. Before any analysis can be performed, the raw text data needs to be cleaned and transformed into a structured format suitable for NLP algorithms.\n",
    "\n",
    "This lab will guide you through implementing a text preprocessing pipeline following these key steps:\n",
    "\n",
    "- Text Acquisition and Initial Cleaning\n",
    "- Tokenization and Structural Decomposition\n",
    "- Normalization and Standardization\n",
    "- Noise Filtering and Feature Selection\n",
    "- Feature Extraction and Representation\n",
    "- Validation and Iterative Refinement\n",
    "\n",
    "You'll apply these steps to transform messy, unstructured hotel reviews into clean, analyzable data that can reveal meaningful patterns and insights.\n",
    "\n",
    "## Step 0: Imports and Data\n",
    "\n",
    "First, run the provided code cell to import the necessary libraries and download required NLTK resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Import specific NLTK modules\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the hotel reviews dataset\n",
    "hotel_reviews = pd.read_csv('hotel_reviews.csv')\n",
    "hotel_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Calculate basic text statistics\n",
    "hotel_reviews['review_length'] = hotel_reviews['review_text'].apply(len)\n",
    "hotel_reviews['word_count'] = hotel_reviews['review_text'].apply(lambda x: len(str(x).split()))\n",
    "hotel_reviews['sentence_count'] = hotel_reviews['review_text'].apply(lambda x: len(sent_tokenize(str(x))))\n",
    "\n",
    "# Create visualizations to understand the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(hotel_reviews['word_count'], bins=20)\n",
    "plt.title('Distribution of Review Lengths')\n",
    "plt.xlabel('Word Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='rating', y='word_count', data=hotel_reviews)\n",
    "plt.title('Review Length by Rating')\n",
    "plt.xlabel('Rating (1-5)')\n",
    "plt.ylabel('Word Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Text Cleaning and Tokenization\n",
    "Implement functions to clean the text and break it down into analyzable units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "hotel_reviews = pd.read_csv(\"hotel_reviews.csv\")  # change path if needed\n",
    "\n",
    "# Tokenize the reviews into sentences and word tokens\n",
    "hotel_reviews['sentences'] = hotel_reviews['review_text'].apply(lambda x: [])  # Skipping sentence tokenization due to offline constraints\n",
    "hotel_reviews['tokens'] = hotel_reviews['review_text'].apply(\n",
    "    lambda text: re.findall(r'\\b\\w+\\b', text.lower()) if isinstance(text, str) else []\n",
    ")\n",
    "\n",
    "# Assign the count of unique word tokens across all reviews\n",
    "all_tokens = []\n",
    "for tokens in hotel_reviews['tokens']:\n",
    "    all_tokens += tokens\n",
    "unique_token_count_pre = len(set(all_tokens))\n",
    "\n",
    "# Create function to clean initial word tokens\n",
    "def clean_text(text_tokens):\n",
    "    \"\"\"\n",
    "    Clean raw text by:\n",
    "        1. Converting to lowercase\n",
    "        2. Removing HTML tags if present\n",
    "        3. Removing special characters\n",
    "        4. Handling common abbreviations\n",
    "\n",
    "    Parameters:\n",
    "    text_tokens: List of word tokens\n",
    "\n",
    "    Returns:\n",
    "    list: cleaned list of tokens\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in text_tokens]\n",
    "    \n",
    "    # Remove punctuation but keep numbers\n",
    "    cleaned = [re.sub(r'[^a-z0-9]', '', token) for token in tokens if re.sub(r'[^a-z0-9]', '', token) != '']\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Apply cleaning to all reviews\n",
    "hotel_reviews['clean_tokens'] = hotel_reviews['tokens'].apply(clean_text)\n",
    "\n",
    "# Run this cell without changes to check results on a sample review\n",
    "sample_idx = 10  # Review at index 10\n",
    "print(\"Original review:\")\n",
    "print(hotel_reviews['review_text'][sample_idx])\n",
    "print(\"\\nTokens (first 20):\")\n",
    "print(hotel_reviews['tokens'][sample_idx][:20])\n",
    "print(\"\\nClean Tokens (first 20):\")\n",
    "print(hotel_reviews['clean_tokens'][sample_idx][:20])\n",
    "print(\"\\nUnique Number of Tokens:\")\n",
    "print(unique_token_count_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Check results on a sample review\n",
    "sample_idx = 10  # Review at index 10\n",
    "print(\"Original review:\")\n",
    "print(hotel_reviews['review_text'][sample_idx])\n",
    "print(\"\\nSentences:\")\n",
    "print(hotel_reviews['sentences'][sample_idx])\n",
    "print(\"\\nTokens (first 20):\")\n",
    "print(hotel_reviews['clean_tokens'][sample_idx][:20])\n",
    "print(\"\\nUnique Number of Tokens:\")\n",
    "print(unique_token_count_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stopword Removal and Normalization\n",
    "Filter out common words that add little analytical value and normalize (lemmatize) the remaining tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords from a list of tokens\n",
    "    \n",
    "    Parameters:\n",
    "    tokens (list): List of word tokens\n",
    "    \n",
    "    Returns:\n",
    "    list: Tokens with stopwords removed\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply stopword removal\n",
    "hotel_reviews['filtered_tokens'] = hotel_reviews['clean_tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper function for lemmatization with POS tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert NLTK POS tags to WordNet POS tags\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize tokens with appropriate POS tags\n",
    "    \n",
    "    Parameters:\n",
    "    tokens (list): List of word tokens\n",
    "    \n",
    "    Returns:\n",
    "    list: Lemmatized tokens\n",
    "    \"\"\"\n",
    "    # Tag tokens with parts of speech\n",
    "    tokens_tagged = pos_tag(tokens)\n",
    "\n",
    "    # Convert to WordNet POS tags\n",
    "    pos_tokens = [(token, get_wordnet_pos(pos)) for token, pos in tokens_tagged]\n",
    "\n",
    "    # Lemmatize with POS tags\n",
    "    lemmatized = [lemmatizer.lemmatize(token, pos) for token, pos in pos_tokens]\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "# Apply lemmatization\n",
    "hotel_reviews['lemmatized_tokens'] = hotel_reviews['filtered_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Calculate stopword reduction percentage\n",
    "# Assign the count of unique word tokens across all reviews after cleaning and normalizing\n",
    "all_tokens = []\n",
    "for tokens in hotel_reviews['lemmatized_tokens']:\n",
    "    all_tokens.extend(tokens)\n",
    "unique_token_count_post = len(set(all_tokens))\n",
    "stopword_reduction_percent = ((unique_token_count_pre - unique_token_count_post) / unique_token_count_pre) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Check results on the sample review\n",
    "print(\"Tokens after stopword removal (first 15):\")\n",
    "print(hotel_reviews['filtered_tokens'][sample_idx][:15])\n",
    "print(\"\\nTokens after lemmatization (first 15):\")\n",
    "print(hotel_reviews['lemmatized_tokens'][sample_idx][:15])\n",
    "print(f\"\\nRemoving stopwords and lemmatizing the text has reduced the vocaburlary by {stopword_reduction_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate N-grams and Analyze Frequent Terms\n",
    "Extract word sequences to capture phrases and identify common topics in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate n-grams from a list of tokens\n",
    "def generate_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "# Generate bigrams and trigrams\n",
    "hotel_reviews['bigrams'] = hotel_reviews['lemmatized_tokens'].apply(lambda x: generate_ngrams(x, 2))\n",
    "hotel_reviews['trigrams'] = hotel_reviews['lemmatized_tokens'].apply(lambda x: generate_ngrams(x, 3))\n",
    "\n",
    "# Combine all lemmatized tokens into a single list\n",
    "all_tokens = [token for tokens in hotel_reviews['lemmatized_tokens'] for token in tokens]\n",
    "\n",
    "# Calculate token frequencies\n",
    "token_freq = pd.Series(Counter(all_tokens)).sort_values(ascending=False)\n",
    "\n",
    "# Combine all bigrams for frequency analysis\n",
    "all_bigrams = [bigram for bigrams in hotel_reviews['bigrams'] for bigram in bigrams]\n",
    "\n",
    "# Calculate bigram frequencies\n",
    "bigram_freq = pd.Series(Counter(all_bigrams)).sort_values(ascending=False)\n",
    "\n",
    "# Visualize top 20 tokens and bigrams\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "token_freq[:20].plot(kind='bar', title=\"Top 20 Words in Reviews\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "bigram_freq[:20].plot(kind='bar', title=\"Top 20 Bigrams in Reviews\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Separate reviews by sentiment\n",
    "positive_reviews = hotel_reviews[hotel_reviews['rating'] >= 4]\n",
    "negative_reviews = hotel_reviews[hotel_reviews['rating'] <= 2]\n",
    "\n",
    "# Collect tokens by sentiment\n",
    "positive_tokens = [token for tokens in positive_reviews['lemmatized_tokens'] for token in tokens]\n",
    "negative_tokens = [token for tokens in negative_reviews['lemmatized_tokens'] for token in tokens]\n",
    "\n",
    "# Calculate frequencies\n",
    "positive_freq = pd.Series(Counter(positive_tokens)).sort_values(ascending=False)\n",
    "negative_freq = pd.Series(Counter(negative_tokens)).sort_values(ascending=False)\n",
    "\n",
    "# Visualize positive token frequency\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "positive_freq[:20].plot(kind='bar', title=\"Top 20 Words in Positive Reviews\")\n",
    "\n",
    "# Visualize negative token frequency\n",
    "plt.subplot(2, 1, 2)\n",
    "negative_freq[:20].plot(kind='bar', title=\"Top 20 Words in Negative Reviews\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Sentiment Analysis to Preprocessed Text\n",
    "Use the preprocessed text to analyze sentiment and compare with the explicit ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Join lemmatized tokens back into strings for sentiment analysis\n",
    "hotel_reviews['lemmatized_text'] = hotel_reviews['lemmatized_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Calculate sentiment scores\n",
    "hotel_reviews['sentiment_scores'] = hotel_reviews['lemmatized_text'].apply(sia.polarity_scores)\n",
    "hotel_reviews['sentiment_compound'] = hotel_reviews['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Classify sentiment based on compound score\n",
    "def classify_sentiment(score):\n",
    "    \"\"\"\n",
    "    Classify sentiment based on compound score\n",
    "    \n",
    "    Parameters:\n",
    "    score (float): Compound sentiment score\n",
    "    \n",
    "    Returns:\n",
    "    str: Sentiment classification (positive, negative, or neutral)\n",
    "    \"\"\"\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply classification\n",
    "hotel_reviews['predicted_sentiment'] = hotel_reviews['sentiment_compound'].apply(classify_sentiment)\n",
    "\n",
    "# Create a new column that compares the predicted sentiment with the actual rating\n",
    "def compare_sentiment_with_rating(row):\n",
    "    \"\"\"\n",
    "    Compare predicted sentiment with rating\n",
    "    \n",
    "    Parameters:\n",
    "    row: DataFrame row\n",
    "    \n",
    "    Returns:\n",
    "    str: Match status (match or mismatch)\n",
    "    \"\"\"\n",
    "    # Convert rating to sentiment category\n",
    "    actual_sentiment = 'positive' if row['rating'] >= 4 else ('negative' if row['rating'] <= 2 else 'neutral')\n",
    "    \n",
    "    # Compare with predicted sentiment - find where they match/equal\n",
    "    if row['predicted_sentiment'] == actual_sentiment:\n",
    "        return 'match'\n",
    "    else:\n",
    "        return 'mismatch'\n",
    "\n",
    "hotel_reviews['sentiment_match'] = hotel_reviews.apply(compare_sentiment_with_rating, axis=1) \n",
    "\n",
    "# Calculate sentiment accuracy\n",
    "sentiment_accuracy = (hotel_reviews['sentiment_match'] == 'match').mean() * 100\n",
    "\n",
    "# Analyze mismatches\n",
    "mismatches = hotel_reviews[hotel_reviews['sentiment_match'] == 'mismatch']\n",
    "mismatches_by_rating = mismatches['rating'].value_counts().sort_index()\n",
    "mismatches_by_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Preprocessing Pipeline\n",
    "Analyze how your preprocessing choices affected the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to measure vocabulary size at each step\n",
    "def calculate_vocabulary_sizes(df):\n",
    "    \"\"\"\n",
    "    Calculate vocabulary size at each preprocessing step\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with tokens at different preprocessing stages\n",
    "    \n",
    "    Returns:\n",
    "    dict: Vocabulary sizes for each step\n",
    "    \"\"\"\n",
    "    # Original tokens\n",
    "    original_vocab = set()\n",
    "    for tokens in df['clean_tokens']:\n",
    "        original_vocab.update(tokens)\n",
    "    \n",
    "    # After stopword removal\n",
    "    filtered_vocab = set()\n",
    "    for tokens in df['filtered_tokens']:\n",
    "        filtered_vocab.update(tokens)\n",
    "\n",
    "    # After lemmatization\n",
    "    lemmatized_vocab = set()\n",
    "    for tokens in df['lemmatized_tokens']:\n",
    "        lemmatized_vocab.update(tokens)\n",
    "    \n",
    "    return {\n",
    "        'original': len(original_vocab),\n",
    "        'filtered': len(filtered_vocab),\n",
    "        'lemmatized': len(lemmatized_vocab)\n",
    "    }\n",
    "\n",
    "vocab_sizes = calculate_vocabulary_sizes(hotel_reviews)\n",
    "\n",
    "# Calculate token reduction percentages\n",
    "original_to_filtered = ((vocab_sizes['original'] - vocab_sizes['filtered']) / vocab_sizes['original']) * 100\n",
    "filtered_to_lemmatized = ((vocab_sizes['filtered'] - vocab_sizes['lemmatized']) / vocab_sizes['filtered']) * 100\n",
    "total_reduction = ((vocab_sizes['original'] - vocab_sizes['lemmatized']) / vocab_sizes['original']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Summarize vocabulary size changes\n",
    "print(f\"Original vocabulary size: {vocab_sizes['original']} unique tokens\")\n",
    "print(f\"After stopword removal: {vocab_sizes['filtered']} unique tokens ({original_to_filtered:.1f}% reduction)\")\n",
    "print(f\"After lemmatization: {vocab_sizes['lemmatized']} unique tokens ({filtered_to_lemmatized:.1f}% further reduction)\")\n",
    "print(f\"Total vocabulary reduction: {total_reduction:.1f}%\")\n",
    "\n",
    "# Create a bar chart showing vocabulary size reduction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Original', 'After Stopwords', 'After Lemmatization'], \n",
    "        [vocab_sizes['original'], vocab_sizes['filtered'], vocab_sizes['lemmatized']])\n",
    "plt.title('Vocabulary Size Reduction through Preprocessing')\n",
    "plt.ylabel('Number of Unique Tokens')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cohort_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
